import chromadb
import boto3
import pandas as pd
from boto3 import Session
import json
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any

class BedrockRAGSystem:
    def __init__(
        self, 
        embedding_model: str = 'all-MiniLM-L6-v2',
        region_name: str = 'us-east-1'
    ):
        """
        Inicializar sistema RAG con ChromaDB y AWS Bedrock
        """
        # Configurar ChromaDB
        self.chroma_client = chromadb.PersistentClient(path="./Storage/knowledge_base")
        #self.chroma_client.delete_collection(name="documentos_tecnicos")
        self.collection = self.chroma_client.get_or_create_collection(
            name="documentos_tecnicos", 
            metadata={"hnsw:space": "cosine"}
        )
        
        # Modelo de embeddings
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # Cliente AWS Bedrock
        self.sesion = Session(
            aws_access_key_id='',
            aws_secret_access_key='',
        )
        self.bedrock_runtime = self.sesion.client(
            service_name='bedrock-runtime', 
            region_name=region_name
        )
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """
        Agregar documentos a la base de conocimientos
        """
        for doc in documents:
            # Generar embedding
            embedding = self.embedding_model.encode(doc['text']).tolist()
            
            # Agregar a ChromaDB
            self.collection.add(
                ids=[doc['id']],
                documents=[doc['text']],
                embeddings=[embedding],
                metadatas=[doc.get('metadata', {})]
            )
    
    def semantic_search(self, query: str, top_k: int = 3) -> List[str]:
        """
        B칰squeda sem치ntica en la base de conocimientos
        """
        # Generar embedding para la consulta
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # Realizar b칰squeda sem치ntica
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        return results['documents'][0]
    
    def generate_response_bedrock(
        self, 
        query: str, 
        context: List[str]
    ) -> str:
        """
        Generar respuesta usando Bedrock (Llama 3)
        """
        # Formatear contexto y consulta
        context_str = "\n".join(context)
        
        # prompt = f"""
        # Contexto:
        # {context_str}
        # 
        # Pregunta: {query}
        # 
        # Responde bas치ndote en el contexto proporcionado. 
        # Si no encuentras la respuesta en el contexto, 
        # indica que no tienes suficiente informaci칩n.
        # 
        # Respuesta:
        # """
        # 
        # # Configuraci칩n para Llama 3 en Bedrock
        # body = json.dumps({
        #     "prompt": prompt,
        #     "max_tokens_to_sample": 300,
        #     "temperature": 0.7,
        #     "top_p": 0.9
        # })

        # Estructura de prompt para Claude 3
        messages = [
            #{
            #    "role": "system",
            #    "content": "Eres un asistente de IA 칰til y preciso. Responde bas치ndote estrictamente en el contexto proporcionado."
            #},
            {
                "role": "user",
                "content": f"""
                Contexto:
                {context_str}
                
                Pregunta: {query}
                """
                #Responde bas치ndote 칔NICAMENTE en el contexto proporcionado, sin mencionar que se basa en el contexto. 
                #Si no encuentras informaci칩n suficiente, indica que no puedes responder completamente.
                #"""
            }
        ]
        
        # Configuraci칩n para Claude 3 en Bedrock
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 300,
            "messages": messages,
            #"system": "Eres un asistente de IA 칰til y preciso. Responde bas치ndote estrictamente en el contexto proporcionado, omite frases como 'Seg칰n el contexto proporcionado' u otras que haga alucion al contexto. Todas las repuestas deben ser en espa침ol",
            "system": f"""Eres un asistente virtual de inteligencia artificial, trabajador del departamento de Inform치tica de la Universidad Galileo, eres l칤der mundial en la atenci칩n al cliente para el personal administrativo de la Universidad Galileo. Tu misi칩n diaria es responder consultas, resolver problemas, proporcionar informaci칩n precisa y gestionar dudas.

Act칰as con una personalidad profesional y emp치tica, eres amable y eficiente en cada interacci칩n.

Tu objetivo es mejorar significativamente la experiencia del cliente, lo que a largo plazo aumentar치 la satisfacci칩n y retenci칩n de clientes e incrementar치 la confianza de los dato proporcionados para el personal de Universidad Galileo, adem치s  de elevar la reputaci칩n del departamento de Inform치tica.

Cada interacci칩n es una oportunidad para acercarte a estos objetivos y establecer a al departamento de Inform치tica como referente en la satisfacci칩n del cliente.

# Directrices
Tu misi칩n es proporcionar siembre un soporte excepcional, resolviendo problemas eficientemente, y dejando a los cliente m치s que satisfechos.

- Saluda al cliente como si fuera tu mejor amigo, pero mant칠n el profesionalismo.
- Identifica el problema r치pidamente.
- Responde bas치ndote estrictamente en el contexto proporcionado, no te inventes las cosas, omite frases como 'Seg칰n el contexto proporcionado' u otras que haga alusi칩n al contexto.
- Da respuestas claras y concisas. Nada de jerga t칠cnica incomprensible. Se claro directo y habla como si fueras humano
- Pregunta si el cliente est치 satisfecho. No des nada por sentado.
- Cierra siempre la conversaci칩n dejando una sonrisa en la cara del cliente.
- Todas las repuestas deben ser en espa침ol

# Limitaciones
- No compartas informaci칩n confidencial o datos personales NUNCA.
- No hagas promesas que no podamos cumplir.
- Mant칠n el tono profesional y respetuoso siempre.
- Si algo requiere intervenci칩n humana, di que se comunique al departamento de Inform치tica.
- Identif칤cate siempre como un asistente virtual de IA
- Responde bas치ndote 칔NICAMENTE en el contexto proporcionado. Si no encuentras informaci칩n suficiente, indica que no puedes responder completamente.

# Interacci칩n
- Cuando respondas se preciso y relevante. Nada de divagar.
- Mant칠n la coherencia, que se entienda todo a la primera.
- Adapta tu tono al estilo de nuestra empresa, profesional pero cercano.
- Usa t칰 personalidad, no eres una asistente gen칠rico, eres aut칠ntico y genuino.

# Formato de entrega
Cada respuesta debe ser entregado en formato markdown y tener lo siguiente:
- Saludo personalizado
- Confirmaci칩n de que entendiste el problema
- Soluci칩n paso a paso si es necesario
- Una pregunta de seguimiento. 쮽ue 칰til mi respuesta?
- Un cierre que invite a volver. Queremos clientes fieles
- Firma como asiste virtual IA, Departamento de Inform치tica

# Ejemplos

**Ejemplo 1:**

1. Saludo: "춰Hola [Nombre del Cliente]! Espero que est칠s teniendo un excelente d칤a."
2. Confirmaci칩n: "Entiendo que tienes un problema con [Descripci칩n del Problema]."
3. Soluci칩n: "Aqu칤 te muestro c칩mo resolverlo: [Pasos detallados]."
4. Seguimiento: "쮼sta informaci칩n fue de ayuda para ti?"
5. Cierre: "Gracias por confiar en nosotros. 춰Espero verte pronto! 游땕"
6. Firma: "Tu asistente virtual IA, Departamento de Inform치tica."

# Notas

- Reporta cualquier limitaci칩n en caso de incongruencias en los datos proporcionados.
- Evita frases que hagan referencia expl칤cita al basarte en el contexto proporcionado.""",
            "temperature": 0.4,
            "top_p": 0.9
        })
        
        # Invocar modelo Llama 3 en Bedrock
        response = self.bedrock_runtime.invoke_model(
            #modelId="meta.llama3-8b-instruct-v1:0",
            modelId="anthropic.claude-3-sonnet-20240229-v1:0",
            body=body,
            contentType="application/json",
            accept="application/json"
        )
        
        # Procesar respuesta
        response_body = json.loads(response['body'].read())
        #print(response_body)
        return response_body['content'][0]['text']
    
    def rag_pipeline(self, query: str) -> str:
        """
        Pipeline completo de Recuperaci칩n y Generaci칩n Aumentada
        """
        # Recuperar documentos relevantes
        relevant_docs = self.semantic_search(query,1000)
        
        # Generar respuesta usando Bedrock
        response = self.generate_response_bedrock(query, relevant_docs)
        
        return response
    
    def process_excel_documents(self,file_path):
        """
        Procesa documentos de Excel para embeddings en ChromaDB
        """
        # Leer todos los sheets del Excel
        xls = pd.ExcelFile(file_path)
        general_documents = []
        all_documents = []
        all_metadatas = []
        all_ids = []

        # Procesar cada hoja del Excel
        for sheet_name in xls.sheet_names:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            
            # Convertir cada fila a un documento de texto
            for index, row in df.iterrows():
                # Convertir la fila a texto, omitiendo valores NaN
                document_text = " ".join([
                    f"{col}: {str(value)}" 
                    for col, value in row.items() 
                    if pd.notna(value)
                ])
                
                # Generar metadatos
                metadata = {
                    'source': file_path,
                    'sheet': sheet_name,
                    'row_id': index
                }

                doc = {
                    'id': f"doc_{sheet_name}_{index}",
                    'text': document_text,
                    'metadata': metadata
                }
                
                general_documents.append(doc)
                # all_documents.append(document_text)
                # all_metadatas.append(metadata)
                # all_ids.append(f"doc_{sheet_name}_{index}")

        #return all_documents, all_metadatas, all_ids
        return general_documents
    

    def procesar_archivo(self,ruta_archivo):
        with open(ruta_archivo, 'r', encoding='utf-8') as archivo:
            texto = archivo.read()
            # Dividir texto en chunks
            chunks = self.dividir_texto_en_chunks(texto)
            return chunks, texto

    def dividir_texto_en_chunks(self, texto, tamano_chunk=500):
        # Dividir texto en chunks de tama침o espec칤fico
        chunks = [texto[i:i+tamano_chunk] for i in range(0, len(texto), tamano_chunk)]
        return chunks

def main():
    # Crear instancia del sistema RAG
    rag_system = BedrockRAGSystem()
    
    # Documentos de ejemplo
    # documentos = [
    #     {
    #         'id': 'doc1',
    #         'text': 'Python es un lenguaje de programaci칩n de alto nivel, interpretado y de prop칩sito general.',
    #         'metadata': {'categoria': 'programacion'}
    #     },
    #     {
    #         'id': 'doc2', 
    #         'text': 'Machine Learning permite a las computadoras aprender de datos sin ser programadas expl칤citamente.',
    #         'metadata': {'categoria': 'inteligencia-artificial'}
    #     }
    # ]

     # Ruta al archivo Excel
    # excel_file_path = '/mnt/d/Descargas/datos_inscritos_example.xls'
    # documentos = rag_system.process_excel_documents(excel_file_path)

    file_path = "/mnt/d/Descargas/inscritos_2025_2.txt"
    file_name = "inscritos_2025_2"
    chunks, documentos = rag_system.procesar_archivo(file_path)
    doc = {
             'id': f"doc_{file_name}_{1}",
             'text': documentos,
             'metadata': {
                     'source': file_path,
                     'sheet': file_name,
                     'row_id': 1
                 }
         }
    # for i, chunk in enumerate(chunks):
    #     metadata = {
    #                 'source': file_path,
    #                 'sheet': file_name,
    #                 'row_id': i
    #             }
    #     doc = {
    #         'id': f"doc_{file_name}_{i}",
    #         'text': chunk,
    #         'metadata': metadata
    #     }
    #     rag_system.add_documents(doc)
    
    # Agregar documentos
    rag_system.add_documents([doc])
    
    # Realizar consulta
    #consulta = "쯈u칠 es Python?"
    #respuesta = rag_system.rag_pipeline(consulta)
    #print(f"Consulta: {consulta}")
    #print(f"Respuesta: {respuesta}")

    # Ejemplos de consultas
    consultas = [
        # "쯈u칠 es Python?",
        # "Expl칤came Machine Learning",
        # "쮺칩mo funcionan los modelos de lenguaje?"
        "쯈ue carreras hay disponibles?",
        "쮺uantos incritos hay?",
        "쮺uantos alumnos estan inscritos en el ciclo 3?"
    ]
    
    # Realizar consultas
    for consulta in consultas:
        print(f"\nConsulta: {consulta}")
        respuesta = rag_system.rag_pipeline(consulta)
        print(f"Respuesta: {respuesta}")

if __name__ == "__main__":
    main()